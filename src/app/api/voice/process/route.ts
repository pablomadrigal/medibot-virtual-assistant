import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY || ''
});

// Medical consultation system prompt in Spanish
const getSystemPrompt = (step: string) => {
  const basePrompt = `
Eres un asistente m√©dico virtual profesional dise√±ado para ayudar a pacientes con consultas iniciales en espa√±ol.

INSTRUCCIONES IMPORTANTES:
- Mant√©n respuestas CORTAS y CONCISAS (m√°ximo 2-3 frases)
- S√© emp√°tico y profesional
- Haz UNA pregunta espec√≠fica a la vez
- Nunca proporciones diagn√≥sticos definitivos
- Siempre recomienda consultar con un m√©dico para problemas serios
- Habla √∫nicamente en espa√±ol

RESPONDE DE FORMA BREVE Y NATURAL, como si fueras un doctor real hablando con un paciente.
`;

  switch (step) {
    case 'patient-input':
      return basePrompt + `
PASO ACTUAL: Recopilaci√≥n de informaci√≥n del paciente
OBJETIVO: Recopilar s√≠ntomas, historial m√©dico, medicamentos actuales
PREGUNTAS A HACER:
- S√≠ntomas principales y duraci√≥n
- Historial m√©dico relevante
- Medicamentos actuales
- Alergias conocidas
- Factores de riesgo (viajes, exposici√≥n, etc.)

CUANDO LA INFORMACI√ìN EST√â COMPLETA, di: "He recopilado toda la informaci√≥n necesaria. Ahora proceder√© a realizar la evaluaci√≥n m√©dica."
`;
    case 'doctor-review':
      return basePrompt + `
PASO ACTUAL: An√°lisis m√©dico por el doctor
OBJETIVO: Analizar la informaci√≥n recopilada y proporcionar evaluaci√≥n
ACTIVIDADES:
- Revisar s√≠ntomas reportados
- Evaluar posibles diagn√≥sticos
- Identificar factores de riesgo
- Determinar urgencia de atenci√≥n m√©dica

CUANDO LA EVALUACI√ìN EST√â COMPLETA, di: "He completado la evaluaci√≥n m√©dica. Ahora proceder√© a generar las recomendaciones y prescripci√≥n."
`;
    case 'prescription':
      return basePrompt + `
PASO ACTUAL: Generaci√≥n de prescripci√≥n m√©dica
OBJETIVO: Proporcionar recomendaciones y prescripci√≥n
INCLUIR:
- Diagn√≥stico preliminar
- Recomendaciones de tratamiento
- Medicamentos sugeridos (si aplica)
- Referencias a especialistas (si necesario)
- Instrucciones de seguimiento

CUANDO LA PRESCRIPCI√ìN EST√â COMPLETA, di: "He completado la prescripci√≥n m√©dica. Su consulta ha terminado."
`;
    default:
      return basePrompt;
  }
};

export async function POST(request: NextRequest) {
  try {
    console.log('üîç Voice processing request received');
    
    const formData = await request.formData();
    console.log('üìã FormData parsed successfully');
    
    const audioFile = formData.get('audio') as File;
    console.log('üéµ Audio file:', audioFile ? `Type: ${audioFile.type}, Size: ${audioFile.size} bytes` : 'No audio file');
    
    const historyStr = formData.get('history') as string || '[]';
    const rawHistory = JSON.parse(historyStr);
    
    // Convert 'agent' role to 'assistant' for OpenAI API compatibility and filter out invalid messages
    const conversationHistory = rawHistory
      .filter((msg: any) => msg && (msg.content || msg.text) && (msg.content?.trim() !== '' || msg.text?.trim() !== ''))
      .map((msg: any) => ({
        role: msg.role === 'agent' ? 'assistant' : msg.role,
        content: msg.content || msg.text || '' // Handle both 'content' and 'text' fields
      }));
    
    console.log('üìù Conversation history length:', conversationHistory.length);
    
    const consultationStep = formData.get('step') as string || 'saludo';
    const isText = formData.get('isText') === 'true';
    console.log('üéØ Step:', consultationStep, 'IsText:', isText);

    // Helper functions to extract information from conversation
    const extractSymptoms = (history: any[]) => {
      const symptoms: string[] = [];
      const userMessages = history.filter((msg: { role: string; content?: string; text?: string }) => msg.role === 'user').map((msg: { content?: string; text?: string }) => msg.content || msg.text).join(' ');
      const symptomKeywords = ['dolor', 'fiebre', 'tos', 'dolor de cabeza', 'n√°usea', 'v√≥mito', 'diarrea', 'fatiga', 'mareo'];
      
      for (const keyword of symptomKeywords) {
        if (userMessages.toLowerCase().includes(keyword)) {
          symptoms.push(keyword);
        }
      }
      return symptoms;
    };

    const extractMedicalHistory = (history: any[]) => {
      const userMessages = history.filter((msg: { role: string; content?: string; text?: string }) => msg.role === 'user').map((msg: { content?: string; text?: string }) => msg.content || msg.text).join(' ');
      const historyKeywords = ['historial', 'enfermedad', 'condici√≥n', 'diagn√≥stico', 'tratamiento'];
      
      // Extract sentences that mention medical history
      const sentences = userMessages.split(/[.!?]+/);
      const historySentences = sentences.filter(sentence => 
        historyKeywords.some(keyword => sentence.toLowerCase().includes(keyword))
      );
      
      return historySentences.join('. ');
    };

    const extractMedications = (history: any[]) => {
      const medications: string[] = [];
      const userMessages = history.filter((msg: { role: string; content?: string; text?: string }) => msg.role === 'user').map((msg: { content?: string; text?: string }) => msg.content || msg.text).join(' ');
      const medicationKeywords = ['aspirina', 'paracetamol', 'ibuprofeno', 'melatonina', 'vitaminas', 'medicamento', 'pastilla'];
      
      for (const keyword of medicationKeywords) {
        if (userMessages.toLowerCase().includes(keyword)) {
          medications.push(keyword);
        }
      }
      return medications;
    };

    if (!audioFile) {
      console.log('‚ùå No audio file provided');
      return NextResponse.json({ error: 'No audio file provided' }, { status: 400 });
    }

    console.log(isText ? 'üìù Processing text input...' : 'üé§ Processing voice input...');

    let userText = '';

    if (isText) {
      // 1a. Process text input directly
      userText = await audioFile.text();
    } else {
      // 1b. Convert speech to text using Whisper
      console.log('üé§ Starting Whisper transcription...');
      try {
        const transcription = await openai.audio.transcriptions.create({
          file: audioFile,
          model: 'whisper-1',
          language: 'es'
        });
        userText = transcription.text;
        console.log('‚úÖ Whisper transcription completed');
      } catch (whisperError) {
        console.error('‚ùå Whisper API error:', whisperError);
        throw whisperError;
      }
    }

    console.log(`üë§ Usuario dijo: ${userText}`);

    if (!userText.trim()) {
      return NextResponse.json({ 
        error: 'No pude escuchar bien. ¬øPuede repetir por favor?',
        transcript: '',
        response: 'No pude escuchar bien. ¬øPuede repetir por favor?'
      });
    }

    // 2. Generate response using GPT-4
    console.log('üß† Starting GPT-4 completion...');
    console.log('üìù Raw conversation history:', JSON.stringify(rawHistory, null, 2));
    console.log('üîÑ Processed conversation history:', JSON.stringify(conversationHistory, null, 2));
    
    const messages = [
      { role: 'system', content: getSystemPrompt(consultationStep) }, //TODO PM - Aca no esta haciendo nada, pasa un parametro que nada que ver, deberiamos hacer un enum
      ...conversationHistory.slice(-6), //TODO PM -  Deberiamos ver si hay ua mejor manera que solo pasar los ultimos 6 mensajes para el contexto
      { role: 'user', content: userText }
    ];
    
    console.log('üì§ Messages being sent to GPT-4:', JSON.stringify(messages, null, 2));

    let agentResponse = '';
    try {
      const completion = await openai.chat.completions.create({
        model: 'gpt-4',
        messages: messages as any,
        max_tokens: 150,
        temperature: 0.7
      });

      agentResponse = completion.choices[0].message?.content || 'Lo siento, no pude procesar su consulta.';
      console.log(`ü§ñ Agente respondi√≥: ${agentResponse}`);
    } catch (gptError) {
      console.error('‚ùå GPT-4 API error:', gptError);
      throw gptError;
    }

    // 3. Convert response to speech using TTS
    console.log('üîä Starting TTS conversion...');
    let audioBase64 = '';
    try {
      const speechResponse = await openai.audio.speech.create({
        model: 'tts-1',
        voice: 'alloy',
        input: agentResponse
      });

      // Convert the audio to base64 for sending to frontend
      const audioBuffer = Buffer.from(await speechResponse.arrayBuffer());
      audioBase64 = audioBuffer.toString('base64');
      console.log('‚úÖ TTS conversion completed');
    } catch (ttsError) {
      console.error('‚ùå TTS API error:', ttsError);
      throw ttsError;
    }

    // 4. Determine next consultation step and check if step is complete
    let nextStep = consultationStep;
    let stepComplete = false;
    let analysisData = null;
    const userLower = userText.toLowerCase();
    
    // Check if the AI response indicates step completion
    const completionKeywords = {
      'patient-input': ['informaci√≥n completa', 'datos suficientes', 'informaci√≥n necesaria', 'recopilaci√≥n completa', 'proceder√© a realizar la evaluaci√≥n m√©dica'],
      'doctor-review': ['evaluaci√≥n completa', 'an√°lisis terminado', 'revisi√≥n completada', 'diagn√≥stico preliminar', 'proceder√© a generar las recomendaciones'],
      'prescription': ['prescripci√≥n completa', 'recomendaciones finales', 'tratamiento definido', 'consulta terminada', 'consulta ha terminado']
    };
    
    const currentKeywords = completionKeywords[consultationStep as keyof typeof completionKeywords] || [];
    const responseLower = agentResponse.toLowerCase();
    
    // Check if AI response contains completion keywords
    if (currentKeywords.some(keyword => responseLower.includes(keyword))) {
      stepComplete = true;
    }
    
    // Also check conversation length as backup
    if (consultationStep === 'patient-input' && conversationHistory.length > 8) {
      stepComplete = true;
    } else if (consultationStep === 'doctor-review' && conversationHistory.length > 12) {
      stepComplete = true;
    }
    
    // Generate analysis data when step is complete
    if (stepComplete) {
      try {
        if (consultationStep === 'patient-input') {
          // Call the patient analysis endpoint
          const patientDescription = conversationHistory
            .filter((msg: { role: string; content?: string; text?: string }) => msg.role === 'user')
            .map((msg: { content?: string; text?: string }) => msg.content || msg.text)
            .join(' ');
          
          const analysisResponse = await fetch(`${process.env.NEXT_PUBLIC_APP_URL || 'http://localhost:3000'}/api/ai/patient-analysis`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              patientDescription: patientDescription,
              symptoms: extractSymptoms(conversationHistory),
              medicalHistory: extractMedicalHistory(conversationHistory),
              currentMedications: extractMedications(conversationHistory)
            })
          });
          
          if (analysisResponse.ok) {
            const result = await analysisResponse.json();
            if (result.success) {
              analysisData = result.data;
            }
          }
                 } else if (consultationStep === 'doctor-review') {
           // Call the doctor recommendations endpoint
           const patientAnalysis = ''; // We'll need to pass this from the frontend
           const doctorNotes = conversationHistory
             .filter((msg: any) => msg.role === 'assistant')
             .map((msg: any) => msg.content || msg.text)
             .join(' ');
          
          const recommendationsResponse = await fetch(`${process.env.NEXT_PUBLIC_APP_URL || 'http://localhost:3000'}/api/ai/doctor-recommendations`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              patientAnalysis: JSON.stringify(patientAnalysis),
              doctorNotes: doctorNotes,
              urgency: 'medium'
            })
          });
          
          if (recommendationsResponse.ok) {
            const result = await recommendationsResponse.json();
            if (result.success) {
              analysisData = result.data;
            }
          }
        }
      } catch (error) {
        console.error('Error generating analysis data:', error);
        analysisData = null;
      }
    }
    
    // Only advance step if not complete
    if (!stepComplete) {
      if (consultationStep === 'patient-input') {
        // Stay in patient-input until complete
        nextStep = 'patient-input';
      } else if (consultationStep === 'doctor-review') {
        // Stay in doctor-review until complete
        nextStep = 'doctor-review';
      } else if (consultationStep === 'prescription') {
        // Stay in prescription
        nextStep = 'prescription';
      }
    } else {
      // Advance to next step when complete
      if (consultationStep === 'patient-input') {
        nextStep = 'doctor-review';
      } else if (consultationStep === 'doctor-review') {
        nextStep = 'prescription';
      } else if (consultationStep === 'prescription') {
        nextStep = 'prescription'; // Stay in prescription
      }
    }

    return NextResponse.json({
      transcript: userText,
      response: agentResponse,
      audioBase64: audioBase64,
      nextStep: nextStep,
      stepComplete: stepComplete,
      analysisData: analysisData,
      success: true
    });

  } catch (error) {
    console.error('‚ùå Error processing voice:', error);
    console.error('‚ùå Error details:', {
      name: error instanceof Error ? error.name : 'Unknown',
      message: error instanceof Error ? error.message : String(error),
      stack: error instanceof Error ? error.stack : undefined
    });
    return NextResponse.json({ 
      error: 'Error procesando la voz. Int√©ntelo de nuevo.',
      success: false 
    }, { status: 500 });
  }
}